# First-Small-ETL

Un projet de pipeline ETL (Extract, Transform, Load) minimaliste conÃ§u Ã  des fins d'apprentissage. Ce pipeline extrait des donnÃ©es d'un fichier CSV contenant des informations sur des voitures, les transforme, puis les charge dans une base de donnÃ©es PostgreSQL. Il exÃ©cute Ã©galement des requÃªtes SQL dynamiques et sauvegarde leurs rÃ©sultats.

---

## ğŸš€ FonctionnalitÃ©s principales
- **Extraction** : Lecture des donnÃ©es Ã  partir d'un fichier CSV (`cars.csv`) dans un DataFrame pandas.
- **Transformation** : Nettoyage et prÃ©paration des donnÃ©es pour le chargement (renommage, remplacement de valeurs, etc.).
- **Chargement** : Insertion des donnÃ©es transformÃ©es dans une table PostgreSQL.
- **ExÃ©cution de requÃªtes SQL** : Gestion des requÃªtes SQL dynamiques via des fichiers `.sql` et sauvegarde des rÃ©sultats dans des fichiers CSV.

### ğŸ› ï¸ Outils et technologies utilisÃ©es
- **Python** : Gestion du pipeline ETL.
- **Pandas** : Manipulation des donnÃ©es.
- **SQLAlchemy** : Connexion Ã  PostgreSQL.
- **PostgreSQL** : Stockage des donnÃ©es.
- **pytest** : Ã‰criture et exÃ©cution de tests.
- **dotenv** : Gestion des variables d'environnement.

---

## ğŸ—ºï¸ Diagramme ETL

AperÃ§u visuel du pipeline utilisÃ© dans ce projet :

<div align="center">
<img src="data/assets/etl_diagram.png" alt="Diagramme ETL" width="300">
</div>

---

## ğŸ“¦ Installation

1. **Cloner le dÃ©pÃ´t** :
   ```bash
   git clone https://github.com/Mosdef00/first-small-etl.git
   cd first-small-etl
   ```

2. **Installer les dÃ©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

3. **Configurer PostgreSQL** :
   - CrÃ©ez une base de donnÃ©es avec le nom spÃ©cifiÃ© dans votre fichier `.env`.
   - Assurez-vous que les dÃ©tails de connexion dans le fichier `.env` correspondent.

4. **Ajouter les donnÃ©es d'entrÃ©e** :
   - VÃ©rifiez que le fichier `data/input/cars.csv` existe et respecte la structure prÃ©vue.

---

## ğŸš€ Utilisation

### **ExÃ©cuter le pipeline ETL complet**
Lancez le script principal :
```bash
python src/run.py
```

### **ExÃ©cuter les Ã©tapes individuellement**
- **Extraction** : `python src/extract.py`
- **Transformation** : `python src/transform.py`
- **Chargement** : `python src/load.py`

### **ExÃ©cuter les requÃªtes SQL**
Utilisez `run_queries.py` pour exÃ©cuter les requÃªtes SQL dans `sql/` :
```bash
python src/run_queries.py
```

---

## ğŸ§ª Tests

Le projet inclut une suite complÃ¨te de tests pour valider chaque Ã©tape du pipeline ETL et les fonctionnalitÃ©s associÃ©es.

### **Structure des tests**
Voici la structure des fichiers de tests :
```
tests/
â”œâ”€â”€ conftest.py                      # Configurations globales pour pytest
â”œâ”€â”€ test_extract.py                  # Tests pour la phase d'extraction
â”œâ”€â”€ test_transform.py                # Tests pour la phase de transformation
â”œâ”€â”€ test_load.py                     # Tests pour la phase de chargement
â”œâ”€â”€ test_run.py                      # Test d'intÃ©gration pour le pipeline complet
â”œâ”€â”€ test_run_queries.py              # Tests pour l'exÃ©cution des requÃªtes SQL
â”œâ”€â”€ test_run_queries_integration.py  # Tests d'intÃ©gration pour les requÃªtes SQL
```

### **ExÃ©cuter les tests**
Lancez tous les tests avec la commande suivante :
```bash
pytest tests/
```

Vous pouvez Ã©galement exÃ©cuter un test spÃ©cifique :
```bash
pytest tests/test_transform.py
```

### **Exemple de validation**
- **Test d'intÃ©gration du pipeline** : Assure que les Ã©tapes `extract`, `transform` et `load` sont correctement orchestrÃ©es.
- **Test unitaire pour `extract`** : VÃ©rifie que les donnÃ©es sont correctement lues Ã  partir du fichier CSV.
- **Test unitaire pour `load`** : Valide que les donnÃ©es sont correctement insÃ©rÃ©es dans la base de donnÃ©es PostgreSQL.

### **Logs et erreurs**
Les logs des tests et les messages d'erreur dÃ©taillÃ©s sont affichÃ©s pour faciliter le dÃ©bogage.

---

## ğŸ“Š Exemples de rÃ©sultats des requÃªtes SQL

Voici quelques exemples de rÃ©sultats attendus pour les requÃªtes SQL dans le pipeline :

1. **`max_horsepower.sql`** :
   Cette requÃªte sÃ©lectionne la voiture avec la puissance maximale (`horsepower_hp`) dans la base de donnÃ©es.

   **Exemple de rÃ©sultat** :
   ```
   | car_model        | horsepower_hp |
   |------------------|---------------|
   | Ford Mustang     | 450           |
   ```

2. **`cars_by_transmission.sql`** :
   Cette requÃªte regroupe les voitures par type de transmission (`transmission`) et compte le nombre de voitures dans chaque catÃ©gorie.

   **Exemple de rÃ©sultat** :
   ```
   | transmission | car_count |
   |--------------|-----------|
   | Manual       | 120       |
   | Automatic    | 230       |
   ```

3. **`avg_horsepower.sql`** :
   Cette requÃªte calcule la puissance moyenne (`horsepower_hp`) des voitures en fonction du nombre de cylindres (`cylinders`).

   **Exemple de rÃ©sultat** :
   ```
   | cylinders | avg_horsepower |
   |-----------|----------------|
   | 4         | 150            |
   | 6         | 200            |
   | 8         | 350            |
   ```

4. **`all_mercedes.sql`** :
   Cette requÃªte extrait toutes les voitures de marque "Mercedes" dans la base de donnÃ©es.

   **Exemple de rÃ©sultat** :
   ```
   | car_model        | horsepower_hp | gears | transmission |
   |------------------|---------------|-------|--------------|
   | Mercedes 230     | 130           | 4     | Automatic    |
   | Mercedes 240D    | 90            | 4     | Automatic    |
   | Mercedes 280     | 185           | 3     | Manual       |
   ```

---

## ğŸ“‚ Structure du projet

```
first-small-etl/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ cars.csv
â”‚   â””â”€â”€ assets/
|       â”œâ”€â”€ etl_diagram.md
â”‚       â””â”€â”€ etl_diagram.png
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ query_results/
|   â”œâ”€â”€ max_horsepower.csv
â”‚   â”œâ”€â”€ cars_by_transmission.csv
|   â”œâ”€â”€ all_mercedes.csv
â”‚   â””â”€â”€ avg_horsepower.csv
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ max_horsepower.sql
â”‚   â”œâ”€â”€ cars_by_transmission.sql
|   â”œâ”€â”€ all_mercedes.sql
â”‚   â””â”€â”€ avg_horsepower.sql
|   
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ run_queries.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â”œâ”€â”€ test_load.py
â”‚   â”œâ”€â”€ test_run.py
â”‚   â”œâ”€â”€ test_run_queries.py
â”‚   â”œâ”€â”€ test_run_queries_integration.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ“š Apprentissage

Ce projet m'a permis de :
- Me familiariser avec les concepts fondamentaux des pipelines ETL (Extract, Transform, Load).
- Appliquer les bibliothÃ¨ques Python comme `pandas` pour la manipulation de donnÃ©es et `SQLAlchemy` pour interagir avec une base de donnÃ©es PostgreSQL.
- Comprendre l'importance de structurer les Ã©tapes d'un pipeline de donnÃ©es en modules sÃ©parÃ©s pour faciliter la maintenance et la rÃ©utilisation.
- Apprendre Ã  utiliser Git et GitHub pour gÃ©rer le versionnement du code et documenter un projet de maniÃ¨re professionnelle.
- IntÃ©grer des **tests unitaires et d'intÃ©gration** pour garantir la fiabilitÃ© du pipeline.

Ces apprentissages constituent une base solide pour approfondir mes connaissances dans des outils plus avancÃ©s tels qu'Apache Airflow pour l'orchestration et Docker pour la conteneurisation.

---

## ğŸ¾ AmÃ©liorations futures
- **Apache Airflow** : Automatiser l'orchestration du pipeline.
- **Docker** : Conteneuriser le projet pour maximiser sa portabilitÃ©.
- **Tests avancÃ©s** : Ajouter des tests de performance et de charge.
- **CI/CD** : IntÃ©grer un pipeline CI/CD pour tester et dÃ©ployer rapidement les amÃ©liorations.

---

## ğŸ“ Licence
Ce projet est Ã  des fins Ã©ducatives et est sous licence MIT.