# First-Small-ETL

Un projet de pipeline ETL (Extract, Transform, Load) minimaliste conÃ§u Ã  des fins d'apprentissage. Ce pipeline extrait des donnÃ©es d'un fichier CSV contenant des informations sur des voitures, les transforme, puis les charge dans une base de donnÃ©es PostgreSQL.

---

## ğŸš€ FonctionnalitÃ©s principales
- **Extraction** : Lecture des donnÃ©es Ã  partir d'un fichier CSV (`cars.csv`) dans un DataFrame pandas.
- **Transformation** : Nettoyage et renommage des colonnes, remplacement de valeurs et prÃ©paration des donnÃ©es pour le chargement.
- **Chargement** : Insertion des donnÃ©es transformÃ©es dans une table PostgreSQL.
- **ExÃ©cution de requÃªtes SQL** : Gestion des requÃªtes SQL dynamiques via des fichiers `.sql` et sauvegarde des rÃ©sultats dans des fichiers CSV.
- **ModularitÃ©** : Scripts sÃ©parÃ©s pour chaque Ã©tape (Extraction, Transformation, Chargement, RequÃªtes SQL) et un script principal pour orchestrer l'ensemble du pipeline.

---

## ğŸ› ï¸ PrÃ©requis
Pour exÃ©cuter ce pipeline, vous aurez besoin de :
- **PostgreSQL** installÃ© et en cours d'exÃ©cution.
- **Python** (version 3.x recommandÃ©e).
- **Pip** pour gÃ©rer les paquets Python.
- Un fichier `.env` contenant les variables de connexion suivantes pour PostgreSQL :
  ```
  DB_USER=<votre-utilisateur>
  DB_PASSWORD=<votre-mot-de-passe>
  DB_HOST=<votre-hÃ´te>
  DB_PORT=<votre-port>
  DB_NAME=<votre-nom-de-base-de-donnÃ©es>
  ```
- Les bibliothÃ¨ques Python listÃ©es dans `requirements.txt` (par exemple, `pandas`, `sqlalchemy`, `python-dotenv`).

---

## ğŸ“¦ Installation

1. **Cloner le dÃ©pÃ´t** :
   ```bash
   git clone https://github.com/Mosdef00/first-small-etl.git
   cd first-small-etl
   ```

2. **Installer les dÃ©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

3. **Configurer PostgreSQL** :
   - CrÃ©ez une base de donnÃ©es avec le nom spÃ©cifiÃ© dans votre fichier `.env`.
   - Assurez-vous que les dÃ©tails de connexion dans le fichier `.env` correspondent.

4. **Ajouter les donnÃ©es d'entrÃ©e** :
   - VÃ©rifiez que le fichier `data/input/cars.csv` existe et respecte la structure prÃ©vue.

---

## ğŸš€ Utilisation

### **ExÃ©cuter le pipeline ETL complet**
Lancez le script principal pour exÃ©cuter toutes les Ã©tapes (Extraction, Transformation, Chargement) :
```bash
python src/run.py
```

### **ExÃ©cuter les Ã©tapes individuellement**
- **Extraction** :
  ```bash
  python src/extract.py
  ```
- **Transformation** :
  ```bash
  python src/transform.py
  ```
- **Chargement** :
  ```bash
  python src/load.py
  ```

### **ExÃ©cuter les requÃªtes SQL et sauvegarder les rÃ©sultats**
Utilisez le script `run_queries.py` pour exÃ©cuter les requÃªtes SQL enregistrÃ©es dans des fichiers `.sql` et sauvegarder les rÃ©sultats dans des fichiers CSV :
```bash
python src/run_queries.py
```

- Placez vos fichiers `.sql` dans le dossier `sql/` (par exemple, `sql/max_horsepower.sql`).
- Les rÃ©sultats de chaque requÃªte seront sauvegardÃ©s dans le dossier `query_results/` avec des noms correspondants (par exemple, `query_results/max_horsepower.csv`).

#### **Description des requÃªtes SQL**
Voici une description des requÃªtes SQL prÃ©sentes dans le dossier `sql/` :

1. **`max_horsepower.sql`** :
   - Cette requÃªte sÃ©lectionne la voiture avec la puissance maximale (`horsepower_hp`) dans la base de donnÃ©es.
   - **Exemple de sortie** : La voiture et sa puissance maximale.

2. **`cars_by_transmission.sql`** :
   - Cette requÃªte regroupe les voitures par type de transmission (`Transmission`) et compte le nombre de voitures pour chaque type.
   - **Exemple de sortie** : Nombre de voitures avec transmission manuelle ou automatique.

3. **`avg_horsepower.sql`** :
   - Cette requÃªte calcule la puissance moyenne (`horsepower_hp`) des voitures en fonction du nombre de cylindres (`Cylinders`).
   - **Exemple de sortie** : Cylindres et puissance moyenne correspondante.

---

## ğŸ“‚ Structure du projet

```
first-small-etl/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input/
â”‚       â””â”€â”€ cars.csv          # Fichier de donnÃ©es d'entrÃ©e
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb     # Notebook Jupyter pour l'exploration des donnÃ©es
â”œâ”€â”€ query_results/            # RÃ©sultats des requÃªtes SQL (CSV gÃ©nÃ©rÃ©s)
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ max_horsepower.sql    # Exemple de requÃªte SQL
â”‚   â”œâ”€â”€ cars_by_transmission.sql
â”‚   â””â”€â”€ avg_horsepower.sql
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py            # Gestion de l'extraction des donnÃ©es
â”‚   â”œâ”€â”€ transform.py          # Gestion de la transformation des donnÃ©es
â”‚   â”œâ”€â”€ load.py               # Gestion du chargement des donnÃ©es
â”‚   â”œâ”€â”€ run.py                # Orchestration du processus ETL
â”‚   â”œâ”€â”€ run_queries.py        # ExÃ©cution des requÃªtes SQL et sauvegarde des rÃ©sultats
â”œâ”€â”€ requirements.txt          # DÃ©pendances des paquets Python
â”œâ”€â”€ .env                      # Fichier pour les variables d'environnement
â””â”€â”€ README.md                 # Documentation du projet
```

---

## ğŸ¾ AmÃ©liorations futures
- **Tests** : ImplÃ©menter des tests unitaires et d'intÃ©gration pour chaque Ã©tape ETL afin de garantir la cohÃ©rence des donnÃ©es et la fiabilitÃ© du pipeline.
- **Versionnement** : Introduire un systÃ¨me de versionnement pour le pipeline, permettant un meilleur suivi des modifications et des amÃ©liorations au fil du temps.
- **Gestion des erreurs** : Ajouter une gestion exhaustive des erreurs et un systÃ¨me de journalisation pour traiter les cas particuliers et les Ã©checs imprÃ©vus.
- **Ã‰volutivitÃ©** : Optimiser la performance et l'Ã©volutivitÃ© pour les ensembles de donnÃ©es plus volumineux.
- **Documentation** : Ã‰tendre le README avec des exemples de transformations de donnÃ©es et de requÃªtes SQL pour plus de clartÃ©.

---

## ğŸ“ Licence
Ce projet est Ã  des fins Ã©ducatives et est sous licence MIT.